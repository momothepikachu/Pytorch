{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Models\n",
    "\n",
    "In this notebook, I'll show you how to save and load models with PyTorch. This is important because you'll often want to load previously trained models to use in making predictions or to continue training on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper\n",
    "import fc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAAHTCAYAAAB8/vKtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fec6faa668>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 233,
       "width": 233
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fec6f66cc0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image[0,:].numpy().squeeze());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a network\n",
    "\n",
    "To make things more concise here, I moved the model architecture and training code from the last part to a file called `fc_model`. Importing this, we can easily create a fully-connected network with `fc_model.Network`, and train the network using `fc_model.train`. I'll use this model (once it's trained) to demonstrate how we can save and load models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "\n",
    "model = fc_model.Network(784, 10, [512, 256, 128])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 1.711..  Test Loss: 0.989..  Test Accuracy: 0.665\n",
      "Epoch: 1/2..  Training Loss: 1.043..  Test Loss: 0.767..  Test Accuracy: 0.698\n",
      "Epoch: 1/2..  Training Loss: 0.848..  Test Loss: 0.717..  Test Accuracy: 0.734\n",
      "Epoch: 1/2..  Training Loss: 0.828..  Test Loss: 0.661..  Test Accuracy: 0.748\n",
      "Epoch: 1/2..  Training Loss: 0.747..  Test Loss: 0.624..  Test Accuracy: 0.757\n",
      "Epoch: 1/2..  Training Loss: 0.747..  Test Loss: 0.614..  Test Accuracy: 0.761\n",
      "Epoch: 1/2..  Training Loss: 0.695..  Test Loss: 0.588..  Test Accuracy: 0.776\n",
      "Epoch: 1/2..  Training Loss: 0.691..  Test Loss: 0.560..  Test Accuracy: 0.794\n",
      "Epoch: 1/2..  Training Loss: 0.641..  Test Loss: 0.560..  Test Accuracy: 0.791\n",
      "Epoch: 1/2..  Training Loss: 0.631..  Test Loss: 0.545..  Test Accuracy: 0.797\n",
      "Epoch: 1/2..  Training Loss: 0.629..  Test Loss: 0.589..  Test Accuracy: 0.775\n",
      "Epoch: 1/2..  Training Loss: 0.635..  Test Loss: 0.531..  Test Accuracy: 0.804\n",
      "Epoch: 1/2..  Training Loss: 0.618..  Test Loss: 0.523..  Test Accuracy: 0.808\n",
      "Epoch: 1/2..  Training Loss: 0.593..  Test Loss: 0.504..  Test Accuracy: 0.819\n",
      "Epoch: 1/2..  Training Loss: 0.619..  Test Loss: 0.513..  Test Accuracy: 0.814\n",
      "Epoch: 1/2..  Training Loss: 0.644..  Test Loss: 0.508..  Test Accuracy: 0.815\n",
      "Epoch: 1/2..  Training Loss: 0.607..  Test Loss: 0.491..  Test Accuracy: 0.821\n",
      "Epoch: 1/2..  Training Loss: 0.562..  Test Loss: 0.488..  Test Accuracy: 0.822\n",
      "Epoch: 1/2..  Training Loss: 0.613..  Test Loss: 0.496..  Test Accuracy: 0.821\n",
      "Epoch: 1/2..  Training Loss: 0.564..  Test Loss: 0.489..  Test Accuracy: 0.823\n",
      "Epoch: 1/2..  Training Loss: 0.585..  Test Loss: 0.489..  Test Accuracy: 0.823\n",
      "Epoch: 1/2..  Training Loss: 0.576..  Test Loss: 0.479..  Test Accuracy: 0.828\n",
      "Epoch: 1/2..  Training Loss: 0.560..  Test Loss: 0.464..  Test Accuracy: 0.832\n",
      "Epoch: 2/2..  Training Loss: 0.510..  Test Loss: 0.480..  Test Accuracy: 0.830\n",
      "Epoch: 2/2..  Training Loss: 0.581..  Test Loss: 0.476..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.584..  Test Loss: 0.470..  Test Accuracy: 0.828\n",
      "Epoch: 2/2..  Training Loss: 0.542..  Test Loss: 0.478..  Test Accuracy: 0.824\n",
      "Epoch: 2/2..  Training Loss: 0.549..  Test Loss: 0.471..  Test Accuracy: 0.831\n",
      "Epoch: 2/2..  Training Loss: 0.503..  Test Loss: 0.466..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.526..  Test Loss: 0.460..  Test Accuracy: 0.837\n",
      "Epoch: 2/2..  Training Loss: 0.537..  Test Loss: 0.463..  Test Accuracy: 0.830\n",
      "Epoch: 2/2..  Training Loss: 0.560..  Test Loss: 0.458..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.530..  Test Loss: 0.447..  Test Accuracy: 0.837\n",
      "Epoch: 2/2..  Training Loss: 0.530..  Test Loss: 0.455..  Test Accuracy: 0.833\n",
      "Epoch: 2/2..  Training Loss: 0.550..  Test Loss: 0.473..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.558..  Test Loss: 0.452..  Test Accuracy: 0.836\n",
      "Epoch: 2/2..  Training Loss: 0.520..  Test Loss: 0.475..  Test Accuracy: 0.831\n",
      "Epoch: 2/2..  Training Loss: 0.541..  Test Loss: 0.469..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.533..  Test Loss: 0.452..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.482..  Test Loss: 0.455..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.500..  Test Loss: 0.459..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.549..  Test Loss: 0.447..  Test Accuracy: 0.840\n",
      "Epoch: 2/2..  Training Loss: 0.532..  Test Loss: 0.433..  Test Accuracy: 0.841\n",
      "Epoch: 2/2..  Training Loss: 0.500..  Test Loss: 0.447..  Test Accuracy: 0.839\n",
      "Epoch: 2/2..  Training Loss: 0.528..  Test Loss: 0.455..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.527..  Test Loss: 0.434..  Test Accuracy: 0.841\n"
     ]
    }
   ],
   "source": [
    "fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(10).size())\n",
    "print(torch.arange(10).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading networks\n",
    "\n",
    "As you can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n",
    "\n",
    "The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model: \n",
      "\n",
      " Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ") \n",
      "\n",
      "The state dict keys: \n",
      "\n",
      " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "  (1): Linear(in_features=400, out_features=200, bias=True)\n",
       "  (2): Linear(in_features=200, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest thing to do is simply save the state dict with `torch.save`. For example, we can save it to a file `'checkpoint.pth'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can load the state dict with `torch.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to load the state dict in to the network, you do `model.load_state_dict(state_dict)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty straightforward, but as usual it's a bit more complicated. Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. If I create a model with a different architecture, this fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Network:\n\tWhile copying the parameter named \"hidden_layers.0.weight\", whose dimensions in the model are torch.Size([400, 784]) and whose dimensions in the checkpoint are torch.Size([512, 784]).\n\tWhile copying the parameter named \"hidden_layers.0.bias\", whose dimensions in the model are torch.Size([400]) and whose dimensions in the checkpoint are torch.Size([512]).\n\tWhile copying the parameter named \"hidden_layers.1.weight\", whose dimensions in the model are torch.Size([200, 400]) and whose dimensions in the checkpoint are torch.Size([256, 512]).\n\tWhile copying the parameter named \"hidden_layers.1.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([256]).\n\tWhile copying the parameter named \"hidden_layers.2.weight\", whose dimensions in the model are torch.Size([100, 200]) and whose dimensions in the checkpoint are torch.Size([128, 256]).\n\tWhile copying the parameter named \"hidden_layers.2.bias\", whose dimensions in the model are torch.Size([100]) and whose dimensions in the checkpoint are torch.Size([128]).\n\tWhile copying the parameter named \"output.weight\", whose dimensions in the model are torch.Size([10, 100]) and whose dimensions in the checkpoint are torch.Size([10, 128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-d859c59ebec0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# This will throw an error because the tensor sizes are wrong!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m--> 721\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m    722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tWhile copying the parameter named \"hidden_layers.0.weight\", whose dimensions in the model are torch.Size([400, 784]) and whose dimensions in the checkpoint are torch.Size([512, 784]).\n\tWhile copying the parameter named \"hidden_layers.0.bias\", whose dimensions in the model are torch.Size([400]) and whose dimensions in the checkpoint are torch.Size([512]).\n\tWhile copying the parameter named \"hidden_layers.1.weight\", whose dimensions in the model are torch.Size([200, 400]) and whose dimensions in the checkpoint are torch.Size([256, 512]).\n\tWhile copying the parameter named \"hidden_layers.1.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([256]).\n\tWhile copying the parameter named \"hidden_layers.2.weight\", whose dimensions in the model are torch.Size([100, 200]) and whose dimensions in the checkpoint are torch.Size([128, 256]).\n\tWhile copying the parameter named \"hidden_layers.2.bias\", whose dimensions in the model are torch.Size([100]) and whose dimensions in the checkpoint are torch.Size([128]).\n\tWhile copying the parameter named \"output.weight\", whose dimensions in the model are torch.Size([10, 100]) and whose dimensions in the checkpoint are torch.Size([10, 128])."
     ]
    }
   ],
   "source": [
    "# Try this\n",
    "model = fc_model.Network(784, 10, [400, 200, 100])\n",
    "# This will throw an error because the tensor sizes are wrong!\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved in the checkpoint, along with the state dict. To do this, you build a dictionary with all the information you need to compeletely rebuild the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the checkpoint has all the necessary information to rebuild the trained model. You can easily make that a function if you want. Similarly, we can write a function to load checkpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = fc_model.Network(checkpoint['input_size'],\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
      "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint('checkpoint.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[400, 200, 100]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkp = torch.load('checkpoint.pth')\n",
    "checkp['hidden_layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_layers': [400, 200, 100],\n",
       " 'input_size': 784,\n",
       " 'output_size': 10,\n",
       " 'state_dict': OrderedDict([('hidden_layers.0.weight',\n",
       "               tensor([[ 3.0087e-03, -2.8092e-02, -3.8048e-03,  ..., -2.7016e-02,\n",
       "                        -1.8403e-02,  7.4577e-03],\n",
       "                       [ 1.3757e-02,  1.5869e-02, -9.3371e-03,  ..., -1.7473e-02,\n",
       "                         3.4029e-02,  1.2820e-02],\n",
       "                       [-1.7820e-02,  1.9907e-02,  3.2500e-02,  ...,  1.2849e-02,\n",
       "                         1.8669e-02,  4.4875e-03],\n",
       "                       ...,\n",
       "                       [ 2.9934e-02, -4.9981e-03,  1.9324e-02,  ..., -1.2517e-02,\n",
       "                        -1.5419e-02, -4.3521e-03],\n",
       "                       [ 1.9563e-02, -1.8366e-02,  5.3222e-03,  ..., -3.5564e-02,\n",
       "                         2.8100e-04, -2.1811e-02],\n",
       "                       [ 1.8323e-02,  2.3450e-02,  2.3034e-03,  ..., -6.9385e-03,\n",
       "                         9.7090e-03, -2.4685e-02]])),\n",
       "              ('hidden_layers.0.bias', tensor(1.00000e-02 *\n",
       "                      [-0.9147, -2.5008, -3.5401,  0.8743, -0.6859, -2.8509,  0.0048,\n",
       "                        1.4952, -2.1616, -1.3247, -0.8485,  1.9299, -1.1808, -0.5730,\n",
       "                        2.0032,  2.0121,  1.7103,  1.4071,  1.2243, -1.5985,  2.8372,\n",
       "                       -2.3712, -3.0867, -0.0254, -3.1324, -3.3573, -3.1560, -1.5842,\n",
       "                        1.2108, -2.8945, -1.1570,  1.5933,  2.2145,  1.9704,  0.0683,\n",
       "                        3.3180, -0.1587, -2.5150, -1.6591,  0.4397,  2.7179,  0.6708,\n",
       "                       -2.1556,  0.0789,  0.5962,  0.4240,  2.0352,  3.3533, -0.6200,\n",
       "                       -2.4505, -1.0368,  0.5997, -0.6695, -2.9107,  0.3101, -2.9318,\n",
       "                        0.5262,  1.2162, -1.0676,  1.9110, -2.9322, -2.2096, -0.2742,\n",
       "                        0.4304,  3.3328,  2.6097, -1.9920, -0.5243,  1.6515,  1.8433,\n",
       "                       -1.6501,  0.7919,  2.0981, -1.8806,  3.3365, -2.1972,  1.7369,\n",
       "                        2.5807,  2.4618,  1.4542, -2.7427,  1.0824, -0.0248,  2.8677,\n",
       "                        1.3922,  3.2588,  1.3413,  0.3637,  1.3007, -2.9373, -2.9365,\n",
       "                       -3.2060, -3.1818,  1.1266,  2.0712, -3.3928, -1.3392,  3.1830,\n",
       "                        1.5617,  2.5980, -1.7155,  0.8843,  2.1578, -2.8694, -3.2934,\n",
       "                       -1.3682, -2.5211,  3.1708,  0.9273,  2.8104, -0.1880,  2.1991,\n",
       "                        0.7263, -1.4963, -2.0642,  0.8509,  2.1070, -2.6143, -2.9149,\n",
       "                       -3.1935,  2.1989, -3.5016, -1.0180,  1.2766,  1.9493,  0.7892,\n",
       "                       -1.3725,  1.3944,  3.3929,  1.9987,  0.5428,  2.8705,  2.2535,\n",
       "                       -2.3597,  0.8758,  2.6152,  3.3447, -2.0307, -0.7192, -1.7396,\n",
       "                        0.4954, -1.5616,  1.6142,  2.0611, -1.0725,  3.3674,  0.9295,\n",
       "                       -1.1277,  0.5471, -3.0928,  1.4780,  2.0855, -1.7584, -0.3437,\n",
       "                       -2.7467, -0.9665, -0.3634,  2.2923, -3.1153, -1.9175, -0.8424,\n",
       "                       -1.5047, -1.6587,  2.1004,  3.2360,  1.1406, -1.5892,  0.6980,\n",
       "                       -0.1900,  2.7080, -0.3950,  3.2768, -3.5578, -0.0296,  2.3916,\n",
       "                        2.9467,  1.9186,  2.5565, -3.0361, -2.9061,  1.5020,  3.1946,\n",
       "                       -2.8589, -0.7573, -3.0254,  1.9932, -3.5311,  2.5281, -2.8642,\n",
       "                        1.8535,  1.7356,  0.3239,  2.2898,  0.7839,  1.8065,  1.5889,\n",
       "                        1.2772,  1.0387, -3.5678,  0.0812,  0.8499, -3.4436, -2.2709,\n",
       "                        2.8187, -1.3787, -0.6297,  0.4759,  3.2521, -1.6435, -2.2250,\n",
       "                       -2.4251, -1.2873, -3.4380, -3.3304,  1.6768, -1.4564,  2.5432,\n",
       "                        0.1413, -2.1093, -0.8800, -0.8983,  1.6569, -3.5132, -0.2182,\n",
       "                        2.1261, -2.9387,  2.0810, -2.4347,  2.7169, -0.3388, -2.6707,\n",
       "                       -0.4479, -3.2246, -2.9083, -0.6407,  0.8896,  0.9062, -0.6926,\n",
       "                       -1.3940, -0.0681,  2.0651, -0.0360, -0.3221, -2.8101, -1.3735,\n",
       "                        0.5991,  2.2627, -0.6823,  3.0929, -3.2990,  0.0645,  0.4765,\n",
       "                       -1.4518,  0.8997, -2.4842, -1.1070,  0.6402,  3.3414,  0.6808,\n",
       "                        3.4563,  1.1499, -0.3162,  1.0735, -2.0320, -2.6757,  3.5173,\n",
       "                       -0.2841,  2.4076, -0.1518,  2.7884,  0.3510, -2.5920, -1.7972,\n",
       "                        3.0916, -1.7765,  2.7568, -1.7343, -0.2746,  2.3384,  2.6452,\n",
       "                        2.3319, -0.8649,  3.1597, -0.0979,  1.9645,  2.2283,  2.5360,\n",
       "                        0.8688,  2.6711,  0.3055,  2.0535, -3.3088,  2.6992, -1.0675,\n",
       "                        0.6886,  2.4023,  0.7737, -1.0032,  0.2807, -0.4555, -2.4626,\n",
       "                        2.3104, -2.9341, -2.6961,  3.0487,  1.6948,  0.8933, -1.7863,\n",
       "                        0.9992,  1.4530, -1.9218, -0.8512,  1.3925, -0.9380, -1.8091,\n",
       "                       -2.2404,  1.0120,  1.3154, -3.3396,  1.7237, -1.3967,  2.1310,\n",
       "                        1.5715,  0.8946,  2.8418,  1.6129, -0.9289,  0.3813, -2.8357,\n",
       "                       -0.0318,  1.6184,  0.3470,  0.6908,  3.2951,  3.0847, -2.9811,\n",
       "                        1.8857, -3.2634, -1.1356,  1.6990, -1.3504, -1.9300, -1.8058,\n",
       "                       -0.5736,  1.5268,  3.0413, -2.7680,  3.2811, -1.6548, -2.0686,\n",
       "                        0.4404,  2.6314, -1.4026, -0.1654,  0.5440,  3.3809, -1.7284,\n",
       "                        1.7229, -0.9646,  3.1966,  3.0506,  0.6979,  2.8435,  2.9843,\n",
       "                       -1.9038, -3.4943, -1.6985, -0.2374, -1.5706, -2.3844,  1.7478,\n",
       "                        3.4028, -2.3320, -3.5157, -2.9444, -2.1697,  3.4273, -2.4541,\n",
       "                       -1.4684, -3.2004,  1.7550, -2.3736,  1.0046,  2.5059, -2.0582,\n",
       "                        2.4436,  3.5432,  0.9882, -0.8915, -2.8471, -2.6822,  2.3601,\n",
       "                        0.1914,  0.3040, -0.1752,  2.4857,  2.0979, -2.4254,  0.8538,\n",
       "                       -1.6553])),\n",
       "              ('hidden_layers.1.weight',\n",
       "               tensor([[ 1.2695e-02, -4.9889e-03,  2.6240e-02,  ...,  9.3300e-03,\n",
       "                        -4.1096e-02, -2.6260e-02],\n",
       "                       [-5.1422e-03, -2.8010e-02,  4.9925e-02,  ..., -3.0379e-03,\n",
       "                         7.4250e-03,  1.2537e-02],\n",
       "                       [-3.2541e-02, -2.8235e-02, -2.0503e-02,  ..., -1.9721e-02,\n",
       "                        -1.0208e-02,  2.8524e-02],\n",
       "                       ...,\n",
       "                       [-1.9411e-02,  3.1651e-02, -3.9635e-02,  ...,  3.5617e-02,\n",
       "                         4.8882e-02,  2.8119e-02],\n",
       "                       [-8.9499e-04,  1.4522e-02, -7.3233e-04,  ..., -4.3624e-02,\n",
       "                         1.2000e-02, -2.5212e-02],\n",
       "                       [ 3.8671e-03,  2.4399e-02, -9.3417e-03,  ..., -2.4352e-02,\n",
       "                         4.9343e-02,  3.6285e-02]])),\n",
       "              ('hidden_layers.1.bias', tensor(1.00000e-02 *\n",
       "                      [ 3.3366,  3.8001, -0.3415,  0.5509,  0.4192,  3.4850,  3.1584,\n",
       "                       -4.0919, -1.0104,  0.9823, -2.3815,  0.2635,  1.5768,  2.2618,\n",
       "                       -0.0850, -4.6469,  3.9830,  0.9690, -0.3362, -0.9289,  3.9751,\n",
       "                       -4.6750,  2.7373, -3.6619, -1.4254,  4.5241, -3.3649, -1.9251,\n",
       "                       -0.3982,  1.1991,  4.6895, -0.2624, -1.4421, -2.7091, -3.9643,\n",
       "                       -1.7661, -1.5418, -1.0452, -1.4971, -0.3827, -2.8432, -0.4574,\n",
       "                       -4.6771,  1.4064, -3.0843,  3.4388,  2.8064, -1.1688, -2.4647,\n",
       "                       -1.2776,  0.0089, -4.4958, -4.7228,  3.0586,  1.1006, -2.0479,\n",
       "                       -4.6420,  1.7305, -1.8997, -4.4232, -3.9501, -0.4370,  2.0889,\n",
       "                       -4.8231, -4.5805, -1.6062,  4.9498, -4.4972,  0.7387,  3.4573,\n",
       "                       -0.3125, -1.9981, -0.2831,  2.7517, -3.3633, -2.5831, -3.8827,\n",
       "                       -3.9043,  4.5122, -4.8196,  4.4109,  1.9139,  2.2964,  1.5194,\n",
       "                        3.5115, -3.3768,  1.0388,  2.9375, -4.9960, -2.4428, -1.0728,\n",
       "                        4.0357, -0.0384,  1.6551, -2.9355, -4.1129, -4.2527,  4.8702,\n",
       "                        3.3606,  4.7137, -4.5893, -2.4443,  2.1845,  1.3930,  3.0764,\n",
       "                        1.2703,  0.6405,  0.5596, -4.5472,  0.6123,  2.6518,  4.9813,\n",
       "                       -0.7841,  2.6002,  0.6045, -2.2835, -1.5121,  3.5034, -2.8584,\n",
       "                        2.6936,  2.9960,  1.7359,  0.9994, -2.9501,  1.5324,  0.7880,\n",
       "                        2.7102,  3.1156,  4.6359,  3.2355, -1.9045,  1.3919, -1.9269,\n",
       "                        2.7479, -2.9374, -1.4678, -4.9793,  2.9074,  0.5363,  0.5358,\n",
       "                       -3.9436, -3.7018,  4.2042, -0.3461,  1.9709,  0.4510, -3.8627,\n",
       "                        4.5294,  3.5847,  2.1307,  0.5374,  1.5339,  1.0449, -0.6532,\n",
       "                        1.4224, -2.8423,  1.2415,  4.5812,  0.9037,  3.5507, -4.2711,\n",
       "                        4.9590, -4.7955,  1.8280,  1.9639, -4.6832,  0.7178, -2.0739,\n",
       "                        2.2787,  4.1319,  1.3009,  2.9785, -4.6548, -4.4867, -0.3191,\n",
       "                       -1.3812,  3.5330, -2.3957, -2.6804, -2.2358, -4.7661,  0.9988,\n",
       "                       -4.9861,  2.9427, -2.5064, -1.8301, -3.6937, -4.2713, -0.9198,\n",
       "                       -3.5749,  2.4670,  1.7838,  3.2408, -3.2201,  4.9226, -4.4737,\n",
       "                        4.9406, -2.0212,  4.4548,  0.3793])),\n",
       "              ('hidden_layers.2.weight', tensor(1.00000e-02 *\n",
       "                      [[ 5.9369, -5.2242, -4.5408,  ...,  0.4431, -1.7304, -6.4979],\n",
       "                       [ 5.0746,  3.5136, -0.2836,  ..., -6.3254, -6.1915,  3.3727],\n",
       "                       [-0.5157, -6.3285, -4.7736,  ..., -0.3890,  0.9233,  2.7579],\n",
       "                       ...,\n",
       "                       [ 1.3003,  0.0135,  0.3002,  ...,  6.6905,  5.1209,  0.4059],\n",
       "                       [-0.6573,  4.1544,  3.9077,  ..., -4.1888,  1.2852, -1.4156],\n",
       "                       [-6.8008, -2.9111,  2.8870,  ..., -2.0190, -6.5680, -1.6328]])),\n",
       "              ('hidden_layers.2.bias', tensor(1.00000e-02 *\n",
       "                      [-4.1081, -4.3788, -3.0121, -7.0237,  3.9399, -5.4941,  5.2325,\n",
       "                        4.8476,  4.0406, -2.1334,  5.4856,  1.4783, -4.9039,  3.9690,\n",
       "                       -1.6729, -4.7958,  1.6113,  1.5824, -3.0244, -6.1337,  4.3145,\n",
       "                        0.4453,  1.3970,  4.3939, -4.0416, -6.1215,  4.9590,  6.3742,\n",
       "                        5.7916,  0.3179, -4.0824,  1.3971,  0.5212,  2.4615,  4.7527,\n",
       "                        1.3132, -3.3617,  1.3522, -3.6053,  3.2026,  1.6180, -3.4176,\n",
       "                       -0.7291, -3.6555, -5.9540,  1.4297,  1.8380,  2.5676,  4.3401,\n",
       "                        1.3895, -4.8072, -1.2348,  2.4066, -1.7235,  2.1727, -2.0795,\n",
       "                        4.2404,  2.9834, -5.2288, -5.7794, -1.0299, -1.9453, -4.7188,\n",
       "                       -3.4074, -2.8010, -5.8310, -5.0514,  0.4325, -3.0520,  1.4997,\n",
       "                       -5.4797, -5.1264,  1.5819,  1.4890,  5.4510, -4.8588,  2.6163,\n",
       "                       -2.6775, -6.3443, -4.2556,  2.9840, -2.1695, -4.5875,  2.4615,\n",
       "                       -5.3126,  5.2848, -2.7172, -1.9950, -1.5548,  4.9925, -4.2611,\n",
       "                       -0.8900, -3.8263, -1.1184,  4.8021, -0.5410,  4.0153,  2.5405,\n",
       "                        1.7812, -2.9588])),\n",
       "              ('output.weight', tensor(1.00000e-02 *\n",
       "                      [[ 0.0340,  9.8227,  2.3983, -3.9029, -3.5374, -8.8368,  5.1956,\n",
       "                        -4.4274,  6.7506,  1.1027,  4.2295, -1.6219, -3.8019,  0.8342,\n",
       "                         4.9276,  0.3153, -9.8528, -1.4947, -8.4592, -3.3198,  5.9294,\n",
       "                         9.4975, -4.8363,  1.7416,  8.5357, -3.0349,  1.8298, -0.5126,\n",
       "                         6.6003,  5.6636, -6.8967,  9.3081, -7.3596,  4.9126, -4.7488,\n",
       "                         3.0364, -8.1148,  2.6679,  3.4523, -7.3986,  1.2715,  0.4590,\n",
       "                        -8.0555,  0.1291,  9.5028, -3.0013,  2.3573,  1.1269, -6.8720,\n",
       "                        -2.3663,  7.1861, -3.8648,  4.6654,  9.0677,  6.0155, -2.0186,\n",
       "                         8.3668, -7.2858, -2.5993, -5.7645, -1.6346,  2.4333,  7.4409,\n",
       "                        -7.2258,  7.9732,  6.6208,  5.2858, -9.0345,  0.4805, -2.3501,\n",
       "                        -4.1178,  2.3262, -5.3500, -3.0608, -4.7447, -9.6179, -8.8777,\n",
       "                         3.4444, -0.5496, -1.3267,  0.2738, -7.3475, -1.8077, -2.4058,\n",
       "                         1.2107,  3.3895, -4.3064,  6.1975,  8.5882, -3.9759, -5.0761,\n",
       "                        -1.3620,  8.6318, -7.9469, -3.5940, -8.9028,  8.4187,  4.4934,\n",
       "                         9.3166,  5.9759],\n",
       "                       [ 5.6258, -0.9185,  8.0795, -5.1741,  1.1743, -0.4990, -8.6639,\n",
       "                        -1.6804, -3.3815, -3.9557, -6.4017, -2.3947,  7.6639, -1.8070,\n",
       "                         7.9474, -2.5810, -6.1060, -2.6119,  6.6654, -7.6551, -3.7769,\n",
       "                         0.4818, -2.6219, -3.0408, -2.6465, -4.9775,  7.1750, -0.4185,\n",
       "                        -9.6187, -7.5491, -7.5633,  2.7367,  3.5341, -4.6458, -1.2601,\n",
       "                         8.3715,  1.4105, -8.1173, -5.7342,  2.5909,  7.8950,  4.7785,\n",
       "                         1.7517, -7.7187,  9.1156,  1.3814,  7.7059, -5.2830, -3.7873,\n",
       "                         7.3196,  7.3744, -9.5312,  4.4086, -6.2506, -2.1899,  2.5314,\n",
       "                         0.2162, -6.6097,  2.8120, -6.4361,  3.3629,  3.2008, -9.6672,\n",
       "                         3.1195, -3.4307, -0.1798,  9.1525,  9.8974, -9.3986, -9.3546,\n",
       "                         9.0456,  7.6416,  7.9037, -0.1610,  7.2299,  0.7355,  7.4070,\n",
       "                         0.4420,  1.9910, -7.8251,  5.1792,  5.9687,  5.0287,  9.8186,\n",
       "                         3.0635,  6.7342,  5.9019,  1.6416, -4.4408,  8.0657,  6.4590,\n",
       "                         5.9556, -2.8788, -7.3896, -1.0335,  8.2850,  9.9418, -9.8811,\n",
       "                        -6.4807, -5.8633],\n",
       "                       [-2.9760,  0.4347,  8.5571, -7.6078, -8.9521,  0.6066, -5.1557,\n",
       "                        -5.8404,  2.7255,  0.5362,  7.1811,  3.2473, -7.5010,  9.3088,\n",
       "                         3.0130,  0.8394,  2.5997, -7.7372,  4.8262, -3.3229,  1.7689,\n",
       "                        -2.9625,  1.0917, -3.8880, -5.3044,  7.6528,  3.2219, -3.1545,\n",
       "                        -0.5172,  7.5491, -3.2876,  5.0160, -4.9946, -9.9034,  9.7031,\n",
       "                         2.1077,  9.0037,  8.3557,  5.9173, -9.1778,  2.4449, -2.7182,\n",
       "                         3.4351,  3.1982, -3.8329,  8.0234,  8.8836, -4.8897, -2.4914,\n",
       "                         2.3891,  6.4113, -6.2854,  8.9812,  9.4462,  6.4406, -8.1470,\n",
       "                         5.2319, -9.4417,  9.5009,  4.0110,  2.8689,  2.4608, -8.4993,\n",
       "                        -3.0981, -1.0001, -0.5416,  9.2566,  6.2824, -2.1935, -8.2953,\n",
       "                        -4.1201,  3.4411, -1.3181,  9.9640, -9.4898,  9.1630,  1.6208,\n",
       "                        -6.2472, -0.0933,  3.2241, -0.6368, -7.4593,  6.0754,  4.2020,\n",
       "                         8.9546,  8.2466,  6.4946,  7.1188,  3.3248,  1.5759, -6.9628,\n",
       "                        -2.5377,  2.5662,  0.1122,  9.1317,  9.1280, -0.0202,  5.3303,\n",
       "                         6.9669, -1.1188],\n",
       "                       [ 2.8786,  5.4222, -5.8548, -5.2319, -4.3599,  4.5812, -3.1813,\n",
       "                         2.0288, -4.7688,  4.2769, -7.5130,  0.4356,  5.3897,  3.8782,\n",
       "                        -6.5118,  2.3716,  6.4324,  4.4779,  3.0940,  9.7433, -7.6329,\n",
       "                        -8.8190, -0.6260,  6.6239,  4.0829, -6.2439, -9.6782, -7.9622,\n",
       "                        -8.6651, -4.3076,  4.8962, -8.3920,  7.3945, -2.2569, -3.7151,\n",
       "                        -9.0348, -7.0257, -9.6565, -9.9162, -3.7270,  4.7288,  0.8027,\n",
       "                         5.5639, -9.2580, -6.1704,  3.3918, -5.2121,  9.8977,  2.9939,\n",
       "                        -7.1424,  8.9918, -5.9018,  3.9175, -6.2675,  5.9817,  4.3672,\n",
       "                         1.3563, -0.3762, -7.8279,  4.9947, -9.4423,  6.9703,  5.3492,\n",
       "                         7.3666,  1.0688,  9.4611,  3.4097,  6.4073, -9.2815,  2.2840,\n",
       "                         9.3781, -9.0120, -1.6416, -5.4336,  9.9143, -3.6630, -2.4993,\n",
       "                        -0.5304, -8.7032,  2.5784,  2.4773, -7.1352,  0.3137, -9.4829,\n",
       "                        -1.2099,  3.5151, -8.2921, -6.8441, -4.1596,  2.9706,  2.1063,\n",
       "                        -3.3288,  3.9364, -4.1907,  6.7013, -5.6575,  8.3075,  8.8109,\n",
       "                         3.1902, -3.9006],\n",
       "                       [-9.1418,  8.4014,  9.0394, -6.5534, -9.9744,  6.5523, -0.8718,\n",
       "                        -7.1021, -7.9698,  2.0267,  0.2976,  9.2280,  0.4635, -9.8833,\n",
       "                        -0.1023, -9.6839,  5.0147, -9.1162, -1.6559, -6.1416, -0.9367,\n",
       "                        -2.9715,  1.1958,  3.6905,  3.3807, -3.9215,  9.1277,  1.3999,\n",
       "                         9.4928,  2.4157,  3.9127, -8.0269,  4.1940, -3.9804, -1.1094,\n",
       "                         2.7945, -3.0754, -3.6185, -5.0722, -2.4023,  4.1868, -5.4134,\n",
       "                         9.4538,  4.9397,  9.4915, -0.3461, -6.9363,  5.9133,  9.2135,\n",
       "                        -0.7396,  2.8835,  0.4369,  0.9283, -8.6597,  8.6005, -6.2323,\n",
       "                         6.3666, -6.7682, -4.3197,  7.9732, -5.8662, -6.1706,  5.2552,\n",
       "                        -1.0325,  2.1593,  0.0073, -0.7762, -7.2228, -8.4063,  5.8800,\n",
       "                        -9.0608,  1.1862, -2.5267, -5.4297, -1.9564,  1.2994, -8.1211,\n",
       "                        -7.5867,  4.4382,  9.0313, -9.0965,  3.0664,  0.8458,  0.3749,\n",
       "                        -0.9599, -7.3191, -2.6947,  8.3338, -8.5349, -4.9518,  1.8251,\n",
       "                         7.1078,  3.2132,  1.3179, -1.5473,  9.4481, -4.9852, -7.8925,\n",
       "                         3.4592,  2.6241],\n",
       "                       [ 7.9532,  0.8262, -3.3172, -5.2849, -3.6493,  8.4109,  8.9908,\n",
       "                         5.9767, -6.4266,  1.7939, -3.0341, -2.4109, -9.0171, -5.3800,\n",
       "                        -3.8677, -0.5578,  9.0917, -4.8143, -8.2887, -1.6785,  7.9866,\n",
       "                        -9.4268, -1.4222,  3.4629, -5.4230, -4.0647, -4.8387, -0.2938,\n",
       "                         8.9497,  3.7045, -1.1519, -3.9509, -5.0612,  8.0081, -0.6437,\n",
       "                        -0.6449,  5.2595,  3.8130,  2.0812, -4.3219,  3.5756, -2.6644,\n",
       "                        -4.8794, -6.1992, -9.9310, -9.9056,  9.4841,  5.7612,  9.7593,\n",
       "                        -2.1041, -8.6236,  8.8007,  2.2489,  1.3683,  2.3434, -6.6251,\n",
       "                         5.1869,  1.4490, -1.5020,  6.5597,  4.4353,  6.5974, -3.9082,\n",
       "                         2.4552, -5.9324, -5.1098, -0.4262,  8.3841, -5.3495, -9.9922,\n",
       "                         3.5618, -3.7256, -3.3042, -4.2562,  4.6254, -6.7977,  5.7335,\n",
       "                         0.0359,  1.9778, -5.0045,  1.2238,  1.1541,  1.9475, -2.8840,\n",
       "                         6.2056, -3.8298,  9.6647, -3.9999,  4.9006, -9.8042, -5.3488,\n",
       "                         4.2885, -8.9841, -5.1583, -0.8928, -0.4289,  1.9931,  9.0493,\n",
       "                        -2.7713,  0.9526],\n",
       "                       [ 7.1511, -5.9732, -2.3406,  0.9035,  2.7344, -7.0282, -3.3979,\n",
       "                         4.2576, -4.0298,  0.4997, -5.6295, -3.9756,  7.4868, -4.2619,\n",
       "                         2.6182, -8.3453, -0.2959, -6.8764,  0.5331, -5.0227, -8.4263,\n",
       "                         6.0915, -8.1224, -2.9538,  3.1602, -0.4746, -0.6631, -0.6228,\n",
       "                        -3.2025, -5.6974, -6.4604,  8.7043,  0.9221,  9.5272,  5.5686,\n",
       "                         4.0168,  7.2487,  1.5621,  0.9816,  4.1660, -5.9930, -1.5859,\n",
       "                        -1.4963,  3.1988, -3.6832, -3.7493, -8.9313, -8.6882,  7.3528,\n",
       "                        -8.6326,  7.4961, -5.0531, -5.6498,  9.4712,  1.4337, -6.1605,\n",
       "                         9.4280,  8.0868,  3.4663, -9.0720, -5.5425,  1.6189,  8.2885,\n",
       "                        -7.3604, -2.7216, -9.6169,  1.5368,  4.0541, -9.6253, -3.3921,\n",
       "                         8.7216, -0.5045, -0.5019, -5.5727,  7.1410,  1.7237,  4.5537,\n",
       "                        -0.8175, -4.0662,  6.5814, -4.0794, -2.0229,  7.2885,  8.2256,\n",
       "                        -7.1574, -3.2539, -2.0624, -4.0005,  1.2827,  1.5540,  4.1423,\n",
       "                        -4.1641,  2.0216, -8.1320,  4.1521,  4.7943, -1.5009,  8.4975,\n",
       "                        -4.1108,  0.8759],\n",
       "                       [-5.8326,  8.4851, -3.2896, -8.6277, -6.1925, -1.2917, -7.9703,\n",
       "                         9.3632,  3.6477,  1.8032,  5.3861,  6.7542,  8.9377, -8.2144,\n",
       "                        -6.6305,  1.6639,  9.5504, -6.1480,  0.2770, -1.2859,  6.7369,\n",
       "                         0.1488, -5.6086, -9.5687, -7.8535,  5.3545,  0.5474,  3.6123,\n",
       "                         5.5039,  9.0872, -8.7725,  9.2952,  0.5153, -6.6479,  8.9476,\n",
       "                        -1.8295, -0.3531, -1.4934,  2.9403,  7.0585,  1.6031, -9.3118,\n",
       "                         4.0904,  9.5270, -0.4660,  7.2985, -3.3688, -1.2266,  6.1841,\n",
       "                         3.6700,  3.7493,  5.5472, -4.8798,  0.4396, -4.8426,  3.6349,\n",
       "                        -9.7527, -7.7441, -1.0444, -9.8869, -6.9016, -6.6038, -0.6454,\n",
       "                         0.7445, -6.4434,  5.8813,  7.3954, -6.5548,  9.3743,  7.2208,\n",
       "                         9.8980,  4.3049, -6.7070, -2.1896, -1.8775,  1.4974,  1.0455,\n",
       "                        -4.3464,  7.7012, -4.4394, -2.6141, -3.6410, -8.6736, -5.5105,\n",
       "                        -4.4365, -1.6890,  3.6184,  6.6299, -4.7655, -2.5353, -0.5946,\n",
       "                         8.8666,  7.5885,  6.0768,  5.5801,  4.7987,  9.8871, -7.7333,\n",
       "                        -7.7209, -7.5037],\n",
       "                       [ 6.4583, -9.1710,  7.4795,  0.5226,  3.5703,  5.3741,  4.1571,\n",
       "                         4.7744, -4.9122, -8.5666,  7.0032,  6.6193, -9.9676,  6.7465,\n",
       "                         8.2895, -6.2320,  5.9794,  1.7574, -9.9448,  8.5105, -2.3335,\n",
       "                        -8.9229, -4.3469, -2.2316, -5.7178, -5.6829, -3.6642,  8.0892,\n",
       "                        -5.3751,  1.5940, -1.0238,  7.7751, -9.8419,  0.4744, -5.3842,\n",
       "                         9.8851,  7.5773, -4.3174, -0.1182, -4.6895, -7.0997,  6.1761,\n",
       "                        -6.9334, -7.7562,  8.5617,  1.5631,  8.2741,  3.0743, -5.5999,\n",
       "                         5.6650, -0.3248,  2.4554, -3.8370,  2.9550, -8.8403,  7.5537,\n",
       "                        -8.5356, -0.0890, -6.8540,  2.0716,  4.9075, -1.6139, -6.7885,\n",
       "                         6.0795, -9.3740, -5.7451,  3.2676, -8.7148, -2.4368, -7.8785,\n",
       "                         0.9231,  8.7376, -1.1201, -6.1311,  3.6098,  3.1888,  9.7918,\n",
       "                         5.1491, -8.2978, -1.3049,  6.9784,  3.5105, -8.9584, -4.4079,\n",
       "                        -1.3723,  5.0732, -9.6731, -0.7931, -2.9156,  3.1700, -8.6424,\n",
       "                         7.4841,  7.2612, -8.3951,  6.5825, -0.6376,  2.2386,  1.1052,\n",
       "                        -4.7614, -8.0197],\n",
       "                       [-8.0206,  1.9264,  8.3851,  3.6148,  0.6857,  4.8362, -5.9388,\n",
       "                         5.7192, -4.4794, -5.7061, -2.1673, -1.3902,  7.6928, -2.9923,\n",
       "                         0.1985, -5.8210, -6.4451, -7.3143,  0.5095,  9.8419, -3.8890,\n",
       "                         4.3966,  8.2247, -3.6264, -6.3684,  2.5092,  1.1650,  1.6313,\n",
       "                         2.8286, -4.2734,  6.4076, -2.3025, -0.5420, -3.3437,  2.8713,\n",
       "                        -3.7371, -4.1490,  1.2703, -6.9137, -9.0492,  5.6196, -9.3985,\n",
       "                        -9.9652, -4.4812, -2.4572,  2.7567,  5.3694, -9.6133,  9.3217,\n",
       "                         4.1285,  8.2767, -2.8020, -3.9933,  9.0971, -8.7524, -8.0084,\n",
       "                        -7.1505, -1.8849,  5.7780,  7.9131, -6.3549,  0.5910, -8.0445,\n",
       "                         1.4023,  5.0921, -9.9630,  0.6519, -6.8876, -8.9725,  2.4689,\n",
       "                        -7.1443,  7.6946,  2.6404, -0.5083,  3.8253, -2.2696, -2.3993,\n",
       "                        -3.3609, -3.8043, -7.4281,  6.4080, -6.8046, -2.9951,  8.4818,\n",
       "                        -7.2813,  5.2040, -8.8710,  6.7973, -5.3527, -1.6839, -4.9751,\n",
       "                         3.4671,  3.6850, -5.1158, -4.2463, -3.8636, -9.8215, -0.6306,\n",
       "                        -8.0819,  9.1586]])),\n",
       "              ('output.bias',\n",
       "               tensor([-0.0561, -0.2162,  0.1592,  0.0920, -0.0577, -0.0690,  0.0015,\n",
       "                        0.0562,  0.0849, -0.0891]))])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
